{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade cachetools==5.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295b4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff1379",
   "metadata": {},
   "source": [
    "+ Compiling the code to work in visual code studio interfaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbd8b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a topic for your channel: football boots\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_title</th>\n",
       "      <th>channel_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>CBS Sports Golazo</td>\n",
       "      <td>@cbssportsgolazo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Futbol y Curvas</td>\n",
       "      <td>@futbolycurvas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Selección Española de Fútbol (SeFutbol)</td>\n",
       "      <td>@sefutbol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               channel_title       channel_url\n",
       "508                        CBS Sports Golazo  @cbssportsgolazo\n",
       "558                          Futbol y Curvas    @futbolycurvas\n",
       "166  Selección Española de Fútbol (SeFutbol)         @sefutbol"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import re\n",
    "from googleapiclient.discovery import build\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "# Load trend data file\n",
    "file_path = 'C:/Users/Andrea/Documents/DATA/YT_Channel_Recommender/Database/df_trending.csv'\n",
    "df_trending = pd.read_csv(file_path)\n",
    "df_trending['category_id'] = df_trending['category_id'].astype('object')\n",
    "\n",
    "# Connect my api credentialse\n",
    "api_key = open(\"api_key_1.txt\", \"r\").read()\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Get the video ID for user input\n",
    "def get_video_id(user_input):\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=1,\n",
    "        q=user_input\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    items = response['items']\n",
    "    video_ids = []\n",
    "    for item in items:\n",
    "        if 'id' in item and 'videoId' in item['id']:\n",
    "            video_id = item['id']['videoId']\n",
    "        video_ids.append(video_id)\n",
    "\n",
    "    if len(video_ids) > 0:\n",
    "        return video_ids\n",
    "    else:\n",
    "        print(\"Sorry, I can't find your topic in my recommendations. Let's try another topic :).\")\n",
    "        return None\n",
    "\n",
    "# Get video IDs\n",
    "user_input = input(\"Type a topic for your channel: \")\n",
    "video_ids = get_video_id(user_input)\n",
    "\n",
    "# Get channel details\n",
    "def get_tags_channel(video_ids):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_ids\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    database = []\n",
    "\n",
    "    items = response['items']\n",
    "    for item in items:\n",
    "        video_data = [\n",
    "            item['snippet'].get('channelId'),\n",
    "            item['snippet'].get('channelTitle'),\n",
    "            item['snippet'].get('categoryId'),\n",
    "            item['snippet'].get('title'),\n",
    "            item['snippet'].get('tags'),\n",
    "            item['snippet'].get('description')\n",
    "        ]\n",
    "\n",
    "        channel_url = item['snippet'].get('channelId')\n",
    "        request_1 = youtube.channels().list(\n",
    "            part=\"snippet\",\n",
    "            id=channel_url\n",
    "        )\n",
    "        response_1 = request_1.execute()\n",
    "\n",
    "        channel_data = response_1['items'][0]['snippet']\n",
    "        video_data.append(channel_data.get('customUrl'))\n",
    "\n",
    "        database.append(video_data)\n",
    "\n",
    "    return database\n",
    "\n",
    "# Get channel data\n",
    "database = get_tags_channel(video_ids)\n",
    "\n",
    "# Create DataFrame from channels\n",
    "df_channel = pd.DataFrame(database, columns=['channel_id', 'channel_title', 'category_id', 'title', 'tags', 'description', 'custom_url'])\n",
    "\n",
    "# Clean and process the tags\n",
    "def clean_up(tag_list):\n",
    "    cleaned_tags = []\n",
    "    for tag in tag_list:\n",
    "        tag = str(tag).lower()\n",
    "        tag = re.sub(\"http:\\S+\", \" \", tag)\n",
    "        tag = re.findall(\"[a-z]+\", tag)\n",
    "        cleaned_tags.extend(tag)\n",
    "    return ','.join(cleaned_tags)\n",
    "\n",
    "def tokenize(tags):\n",
    "    return word_tokenize(str(tags))\n",
    "\n",
    "def lemmatize_input(tags):\n",
    "    lem = WordNetLemmatizer()\n",
    "    return [lem.lemmatize(word) for word in tags]\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "all_stopwords = english_stopwords + spanish_stopwords\n",
    "\n",
    "def remove_sw_input(tags):\n",
    "    return [word for word in tags if not word in all_stopwords]\n",
    "\n",
    "def clean_tags(tags):\n",
    "    tags = [tag for tag in tags if tag != ',']\n",
    "    tags = list(set(tags))\n",
    "    return tags\n",
    "\n",
    "df_channel['tags'] = df_channel['tags'].apply(clean_up)\n",
    "df_channel['tags'] = df_channel['tags'].apply(tokenize)\n",
    "df_channel['tags'] = df_channel['tags'].apply(lemmatize_input)\n",
    "df_channel['tags'] = df_channel['tags'].apply(remove_sw_input)\n",
    "df_channel['tags'] = df_channel['tags'].apply(clean_tags)\n",
    "\n",
    "# Filter the Trend DataFrame\n",
    "def filter_trending_df(df_trending, category):\n",
    "    filtered_df_trending = df_trending[df_trending['category_id'] == category].copy()\n",
    "    \n",
    "    if filtered_df_trending.shape[0] < 10:\n",
    "        print(\"Sorry, we found just one channel. Please try again.\")\n",
    "        return None\n",
    "    \n",
    "    return filtered_df_trending\n",
    "\n",
    "category = int(df_channel['category_id'].iloc[0])\n",
    "filtered_df_trending = filter_trending_df(df_trending, category)\n",
    "\n",
    "# NLP cleaning and compare the tags to calculate the score\n",
    "def tokenizer_and_remove_punctuation(row):\n",
    "    tokens = word_tokenize(row['tags'])\n",
    "    return [word.lower() for word in tokens if word.isalnum()]\n",
    "\n",
    "def lemmatize(row):\n",
    "    lem = WordNetLemmatizer()\n",
    "    return [lem.lemmatize(word) for word in row['tags']]\n",
    "\n",
    "def remove_sw(row):\n",
    "    return [word for word in row['tags'] if not word in all_stopwords]\n",
    "\n",
    "def calculate_similarity(tags1, tags2):\n",
    "    set_tags1 = set(tags1)\n",
    "    set_tags2 = set(tags2)\n",
    "    score = 1 - jaccard_distance(set_tags1, set_tags2)\n",
    "    return score\n",
    "\n",
    "def calculate_score(row):\n",
    "    tags_channel = df_channel['tags'][0]\n",
    "    tags_trending = row['tags']\n",
    "    score = sum(calculate_similarity(tags_channel, tags_trending) for word_trending in tags_trending)\n",
    "    return score\n",
    "\n",
    "if filtered_df_trending is not None:\n",
    "    filtered_df_trending['tags'] = filtered_df_trending.apply(tokenizer_and_remove_punctuation, axis=1)\n",
    "    filtered_df_trending['tags'] = filtered_df_trending.apply(lemmatize, axis=1)\n",
    "    filtered_df_trending['tags'] = filtered_df_trending.apply(remove_sw, axis=1)\n",
    "    filtered_df_trending['tags'] = filtered_df_trending['tags'].apply(clean_tags)\n",
    "    filtered_df_trending['score'] = filtered_df_trending.apply(calculate_score, axis=1)\n",
    "\n",
    "    top_3_scores = filtered_df_trending.sort_values('score', ascending=False).head(3)\n",
    "\n",
    "    def get_custom_url():\n",
    "        channel_url = []\n",
    "        for channel_id in top_3_scores['channel_id']:\n",
    "            request = youtube.channels().list(\n",
    "                part=\"snippet\",\n",
    "                id=channel_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "            items = response['items']\n",
    "            for item in items:\n",
    "                custom_url = item['snippet'].get('customUrl')\n",
    "                channel_url.append(custom_url)\n",
    "        return channel_url\n",
    "\n",
    "    channel_url = get_custom_url()\n",
    "    \n",
    "top_3_scores['channel_url'] = channel_url\n",
    "top_3_scores = top_3_scores.drop(columns=['channel_id', 'category_id', 'title', 'tags', 'description', 'score'])\n",
    "top_3_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some video recommendation from the keywords\n",
    "video_reccomendation = filtered_df_trending.sort_values('score', ascending=False).head(5).drop(columns=['channel_id', 'channel_title', 'category_id', 'tags', 'description','score'])\n",
    "video_reccomendation = video_reccomendation.rename(columns={'title': 'Video recommendation / recomendación de video'})\n",
    "video_reccomendation = video_reccomendation[['Video recommendation / recomendación de video']]\n",
    "video_reccomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf21566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output as HTML\n",
    "import pandas as pd\n",
    "from html import escape\n",
    "\n",
    "# Crea el documento HTML\n",
    "doc = \"<!DOCTYPE html>\\n\"\n",
    "doc += \"<html>\\n\"\n",
    "doc += \"<head>\\n\"\n",
    "doc += \"<style>\\n\"\n",
    "doc += \"table { border-collapse: collapse; width: 100%; }\\n\"\n",
    "doc += \"th, td { text-align: left; padding: 8px; }\\n\"\n",
    "doc += \"th { background-color: #f2f2f2; }\\n\"\n",
    "doc += \"tr:hover { background-color: #f5f5f5; }\\n\"\n",
    "doc += \"</style>\\n\"\n",
    "doc += \"</head>\\n\"\n",
    "doc += \"<body>\\n\"\n",
    "doc += \"<h1>YouTube Channel Recommender</h1>\\n\"\n",
    "doc += \"<p>Top 3 Recommended Channels:</p>\\n\"\n",
    "doc += \"<table>\\n\"\n",
    "doc += \"<thead>\\n\"\n",
    "doc += \"<tr>\\n\"\n",
    "doc += \"<th>Channel Title</th>\\n\"\n",
    "doc += \"<th>Custom URL</th>\\n\"\n",
    "doc += \"</tr>\\n\"\n",
    "doc += \"</thead>\\n\"\n",
    "doc += \"<tbody>\\n\"\n",
    "\n",
    "for _, row in top_3_scores.iterrows():\n",
    "    channel_title = escape(row['channel_title'])\n",
    "    channel_url = escape(row['channel_url'])\n",
    "    doc += \"<tr>\\n\"\n",
    "    doc += f\"<td>{channel_title}</td>\\n\"\n",
    "    doc += f\"<td><a href='{channel_url}' target='_blank'>{channel_url}</a></td>\\n\"\n",
    "    doc += \"</tr>\\n\"\n",
    "\n",
    "doc += \"</tbody>\\n\"\n",
    "doc += \"</table>\\n\"\n",
    "doc += \"</body>\\n\"\n",
    "doc += \"</html>\\n\"\n",
    "\n",
    "with open('output.html', 'w') as f:\n",
    "    f.write(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d6344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
